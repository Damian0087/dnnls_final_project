{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9WiQLHDrj0A9SYN+aeCkY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import numpy as np\n","import math\n","import textwrap\n","import re\n","\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","from transformers import BertTokenizer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bs4 import BeautifulSoup\n","\n"],"metadata":{"id":"c6MTnOv_0vTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdsegboFMsXK"},"outputs":[],"source":["\n","def compute_bleu(reference_text: str, generated_text: str):\n","    smoothie = SmoothingFunction().method4\n","\n","    # Tokenize by splitting on whitespace\n","    ref_tokens = reference_text.split()\n","    gen_tokens = generated_text.split()\n","\n","    # Avoid BLEU errors with empty outputs\n","    if len(gen_tokens) == 0:\n","        return 0.0\n","\n","    return sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothie)\n","\n","def compute_perplexity(loss_value: float):\n","    try:\n","        return math.exp(loss_value)\n","    except OverflowError:\n","        return float(\"inf\")\n"]},{"cell_type":"code","source":["def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n","  \"\"\"\n","  De-normalize the image (if necessary) and show image\n","  \"\"\"\n","  if de_normalize:\n","    new_mean = -img_mean/img_std\n","    new_std = 1/img_std\n","\n","    image = transforms.Normalize(\n","        mean=new_mean,\n","        std=new_std\n","    )(image)\n","  ax.imshow(image.permute(1, 2, 0))"],"metadata":{"id":"beKk7vC9Oicy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n","    \"\"\"\n","    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n","    \"\"\"\n","    # 1. Define the full Google Drive path\n","    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n","    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n","\n","    # Ensure the directory exists before attempting to save\n","    os.makedirs(drive_folder, exist_ok=True)\n","\n","    # 2. Combine the folder and the filename\n","    full_path = os.path.join(drive_folder, filename)\n","\n","    # 3. Create the checkpoint dictionary\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","    }\n","\n","    # 4. Save the dictionary to the Google Drive path\n","    torch.save(checkpoint, full_path)\n","    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n","\n","\n","def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n","    \"\"\"\n","    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n","    \"\"\"\n","    # Define the same Google Drive folder path\n","    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n","    full_path = os.path.join(drive_folder, filename)\n","\n","    # Check if the checkpoint file exists\n","    if not os.path.exists(full_path):\n","        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n","\n","    # Load the checkpoint\n","    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n","\n","    # Restore model state\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # Restore optimizer state (if provided)\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Extract metadata\n","    epoch = checkpoint.get('epoch', 0)\n","    loss = checkpoint.get('loss', None)\n","\n","    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n","\n","    return model, optimizer, epoch, loss\n"],"metadata":{"id":"5rEURSrOOzRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_gdi_text(text):\n","    \"\"\"Parse GDI formatted text into structured data\"\"\"\n","    soup = BeautifulSoup(text, 'html.parser')\n","    images = []\n","\n","    for gdi in soup.find_all('gdi'):\n","        # Debug: print what BeautifulSoup sees\n","\n","        # Method 1: Try to get image attribute directly\n","        image_id = None\n","        if gdi.attrs:\n","            # Check for attributes like 'image1', 'image2', etc.\n","            for attr_name, attr_value in gdi.attrs.items():\n","                if 'image' in attr_name.lower():\n","                    image_id = attr_name.replace('image', '')\n","                    break\n","\n","        # Method 2: Extract from the tag string using regex\n","        if not image_id:\n","            tag_str = str(gdi)\n","            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n","            if match:\n","                image_id = match.group(1)\n","\n","        # Method 3: Fallback - use sequential numbering\n","        if not image_id:\n","            image_id = str(len(images) + 1)\n","\n","        content = gdi.get_text().strip()\n","\n","        # Extract tagged elements using BeautifulSoup directly\n","        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n","        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n","        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n","\n","        images.append({\n","            'image_id': image_id,\n","            'description': content,\n","            'objects': objects,\n","            'actions': actions,\n","            'locations': locations,\n","            'raw_text': str(gdi)\n","        })\n","\n","    return images\n"],"metadata":{"id":"WY6Ij7RzPE4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id):\n","      \"\"\"\n","        This function generates a sequence of tokens using the provided decoder.\n","      \"\"\"\n","      # Ensure the model is in evaluation mode\n","      model.eval()\n","\n","      # 2. SETUP DECODER INPUT\n","      # Start with the SOS token, shape (1, 1)\n","      dec_input = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n","      # hidden = torch.zeros(1, 1, hidden_dim, device=device)\n","      # cell = torch.zeros(1, 1, hidden_dim, device=device)\n","\n","      generated_tokens = []\n","\n","      # 3. AUTOREGRESSIVE LOOP\n","      for _ in range(max_len):\n","          with torch.no_grad():\n","              # Run the decoder one step at a time\n","              # dec_input is (1, 1) hereâ€”it's just the last predicted token\n","              prediction, hidden, cell = model(dec_input, hidden, cell)\n","\n","          logits = prediction.squeeze(1) # Shape (1, vocab_size)\n","          temperature = 0.9 # <--- Try a value between 0.5 and 1.0\n","\n","          # 1. Divide logits by temperature\n","          # 2. Apply softmax to get probabilities\n","          # 3. Use multinomial to sample one token based on the probabilities\n","          probabilities = torch.softmax(logits / temperature, dim=-1)\n","          next_token = torch.multinomial(probabilities, num_samples=1)\n","\n","          token_id = next_token.squeeze().item()\n","\n","          # Check for the End-of-Sequence token\n","          if token_id == eos_token_id:\n","              break\n","\n","          if token_id == 0 or token_id == sos_token_id:\n","              continue\n","\n","            # Append the predicted token\n","          generated_tokens.append(token_id)\n","\n","          # The predicted token becomes the input for the next iteration\n","          dec_input = next_token\n","\n","      # Return the list of generated token IDs\n","      return generated_tokens\n"],"metadata":{"id":"ORRhJ3QBpMQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","    elif isinstance(m, nn.Linear):\n","        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","        nn.init.constant_(m.bias, 0)"],"metadata":{"id":"ntoSFHtAsnBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation( model, data_loader ):\n","  model.eval()\n","  with torch.no_grad():\n","    frames, descriptions, image_target, text_target = next(iter(data_loader))\n","\n","    descriptions = descriptions.to(device)\n","    frames = frames.to(device)\n","    image_target = image_target.to(device)\n","    text_target = text_target.to(device)\n","\n","    # Get all predictions from the model's forward pass\n","    pred_image_content, pred_image_context, predicted_text_logits_k, hidden, cell = model(frames, descriptions, text_target)\n","\n","    # Calculate text loss using predicted_text_logits_k and text_target, similar to the training loop\n","    prediction_flat = predicted_text_logits_k.reshape(-1, tokenizer.vocab_size)\n","    target_labels = text_target.squeeze(1)[:, 1:] # Slice to get [batch, 119]\n","    target_flat = target_labels.reshape(-1)\n","    ce_loss = criterion_text(prediction_flat, target_flat).item()\n","    ppl = compute_perplexity(ce_loss)\n","    #print(\"Perplexity:\", ppl)\n","\n","    figure, ax = plt.subplots(2, 6, figsize=(20, 5), gridspec_kw={'height_ratios': [2, 1.5]})\n","\n","    for i in range(4):\n","      im = frames[0, i, :, :, :].cpu()\n","      show_image(ax[0,i], im )\n","      ax[0,i].set_aspect('auto')\n","      ax[0,i].axis('off')\n","      wrapped_text = textwrap.fill(tokenizer.decode(descriptions[0, i, :], skip_special_tokens=True), width=40)\n","\n","      ax[1,i].text(\n","            0.5, 0.99,\n","            wrapped_text,\n","            ha='center',\n","            va='top',\n","            fontsize=10,\n","            wrap=True\n","        )\n","\n","      ax[1,i].axis('off') # Hide axes for the text subplot\n","\n","    show_image(ax[0,4], image_target[0].cpu())\n","    ax[0,4].set_title('Target')\n","    ax[0,4].set_aspect('auto')\n","    ax[0,4].axis('off')\n","    text_target_for_decode = text_target.squeeze(1)\n","\n","    wrapped_text = textwrap.fill(tokenizer.decode(text_target_for_decode[0], skip_special_tokens=True), width=40)\n","    ax[1,4].text(\n","            0.5, 0.99,\n","            wrapped_text,\n","            ha='center',\n","            va='top',\n","            fontsize=10,\n","            wrap=False)\n","    ax[1,4].axis('off')\n","    output = pred_image_context[0, :, :, :].cpu() # Use predicted context image for visualization\n","    show_image(ax[0,5], output)\n","    ax[0,5].set_title('Predicted')\n","    ax[0,5].set_aspect('auto')\n","    ax[0,5].axis('off')\n","\n","    generated_tokens = generate(model.text_decoder,\n","                                hidden[:,0, :].unsqueeze(1),\n","                                cell[:, 0, :].unsqueeze(1),\n","                                max_len=150,\n","                                sos_token_id=tokenizer.cls_token_id,\n","                                eos_token_id=tokenizer.sep_token_id)\n","\n","    wrapped_text = textwrap.fill(tokenizer.decode(generated_tokens), width=40)\n","\n","    # Pass strings to compute_bleu, not pre-tokenized lists\n","    ref_text_for_bleu = tokenizer.decode(text_target_for_decode[0], skip_special_tokens=True)\n","    pred_text_for_bleu = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","    bleu = compute_bleu(ref_text_for_bleu, pred_text_for_bleu)\n","    #print(\"BLEU-4:\", bleu)\n","\n","    ax[1,5].text(\n","            0.5, 0.99,\n","            wrapped_text,\n","            ha='center',\n","            va='top',\n","            fontsize=10,\n","            wrap=False )\n","    ax[1,5].axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    return bleu, ppl"],"metadata":{"id":"tTO5fHQPsY3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextTaskDataset(Dataset):\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","      num_frames = self.dataset[idx][\"frame_count\"]\n","      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n","\n","      # Pick\n","      frame_idx = np.random.randint(0, 5)\n","      description = self.image_attributes[frame_idx][\"description\"]\n","\n","      return description  # Returning the whole description"],"metadata":{"id":"ZA1zVpQUySnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AutoEncoderTaskDataset(Dataset):\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","        self.transform = transforms.Compose([\n","          transforms.Resize((240, 500)),# Reasonable size based on our previous analysis\n","          transforms.ToTensor(), # HxWxC -> CxHxW\n","        ])\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","      num_frames = self.dataset[idx][\"frame_count\"]\n","      frames = self.dataset[idx][\"images\"]\n","\n","      # Pick a frame at random\n","      frame_idx = torch.randint(0, num_frames-1, (1,)).item()\n","      input_frame = self.transform(frames[frame_idx]) # Input to the autoencoder\n","\n","      return input_frame, # Returning the image"],"metadata":{"id":"hzKFlq7ayTlU"},"execution_count":null,"outputs":[]}]}