{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxih18oFF7GUe5R0FoBzas"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bdsegboFMsXK"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import matplotlib.pyplot as plt\n","import math\n","\n","def compute_bleu(reference_text: str, generated_text: str) -> float:\n","    \"\"\"\n","    Compute BLEU-4 score between reference and generated text.\n","    \"\"\"\n","    smoothie = SmoothingFunction().method4\n","\n","    # Tokenize by splitting on whitespace\n","    ref_tokens = reference_text.split()\n","    gen_tokens = generated_text.split()\n","\n","    # Avoid BLEU errors with empty outputs\n","    if len(gen_tokens) == 0:\n","        return 0.0\n","\n","    return sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothie)\n","\n","def compute_perplexity(loss_value: float) -> float:\n","    \"\"\"\n","    Compute perplexity from cross-entropy loss.\n","    PPL = exp(loss)\n","    \"\"\"\n","    try:\n","        return math.exp(loss_value)\n","    except OverflowError:\n","        return float(\"inf\")\n"]},{"cell_type":"code","source":["def init_weights(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","    elif isinstance(m, nn.Linear):\n","        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","        nn.init.constant_(m.bias, 0)"],"metadata":{"id":"bz5rIINPOH2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n","  \"\"\"\n","  De-normalize the image (if necessary) and show image\n","  \"\"\"\n","  if de_normalize:\n","    new_mean = -img_mean/img_std\n","    new_std = 1/img_std\n","\n","    image = transforms.Normalize(\n","        mean=new_mean,\n","        std=new_std\n","    )(image)\n","  ax.imshow(image.permute(1, 2, 0))"],"metadata":{"id":"beKk7vC9Oicy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n","    \"\"\"\n","    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n","    \"\"\"\n","    # 1. Define the full Google Drive path\n","    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n","    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n","\n","    # Ensure the directory exists before attempting to save\n","    os.makedirs(drive_folder, exist_ok=True)\n","\n","    # 2. Combine the folder and the filename\n","    full_path = os.path.join(drive_folder, filename)\n","\n","    # 3. Create the checkpoint dictionary\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","    }\n","\n","    # 4. Save the dictionary to the Google Drive path\n","    torch.save(checkpoint, full_path)\n","    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n","\n","\n","def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n","    \"\"\"\n","    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n","    \"\"\"\n","    # Define the same Google Drive folder path\n","    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n","    full_path = os.path.join(drive_folder, filename)\n","\n","    # Check if the checkpoint file exists\n","    if not os.path.exists(full_path):\n","        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n","\n","    # Load the checkpoint\n","    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n","\n","    # Restore model state\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # Restore optimizer state (if provided)\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Extract metadata\n","    epoch = checkpoint.get('epoch', 0)\n","    loss = checkpoint.get('loss', None)\n","\n","    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n","\n","    return model, optimizer, epoch, loss\n",""],"metadata":{"id":"5rEURSrOOzRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_gdi_text(text):\n","    \"\"\"Parse GDI formatted text into structured data\"\"\"\n","    soup = BeautifulSoup(text, 'html.parser')\n","    images = []\n","\n","    for gdi in soup.find_all('gdi'):\n","        # Debug: print what BeautifulSoup sees\n","\n","        # Method 1: Try to get image attribute directly\n","        image_id = None\n","        if gdi.attrs:\n","            # Check for attributes like 'image1', 'image2', etc.\n","            for attr_name, attr_value in gdi.attrs.items():\n","                if 'image' in attr_name.lower():\n","                    image_id = attr_name.replace('image', '')\n","                    break\n","\n","        # Method 2: Extract from the tag string using regex\n","        if not image_id:\n","            tag_str = str(gdi)\n","            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n","            if match:\n","                image_id = match.group(1)\n","\n","        # Method 3: Fallback - use sequential numbering\n","        if not image_id:\n","            image_id = str(len(images) + 1)\n","\n","        content = gdi.get_text().strip()\n","\n","        # Extract tagged elements using BeautifulSoup directly\n","        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n","        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n","        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n","\n","        images.append({\n","            'image_id': image_id,\n","            'description': content,\n","            'objects': objects,\n","            'actions': actions,\n","            'locations': locations,\n","            'raw_text': str(gdi)\n","        })\n","\n","    return images\n"],"metadata":{"id":"WY6Ij7RzPE4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id):\n","      \"\"\"\n","        This function generates a sequence of tokens using the provided decoder.\n","      \"\"\"\n","      # Ensure the model is in evaluation mode\n","      model.eval()\n","\n","      # 2. SETUP DECODER INPUT\n","      # Start with the SOS token, shape (1, 1)\n","      dec_input = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n","      # hidden = torch.zeros(1, 1, hidden_dim, device=device)\n","      # cell = torch.zeros(1, 1, hidden_dim, device=device)\n","\n","      generated_tokens = []\n","\n","      # 3. AUTOREGRESSIVE LOOP\n","      for _ in range(max_len):\n","          with torch.no_grad():\n","              # Run the decoder one step at a time\n","              # dec_input is (1, 1) hereâ€”it's just the last predicted token\n","              prediction, hidden, cell = model(dec_input, hidden, cell)\n","\n","          logits = prediction.squeeze(1) # Shape (1, vocab_size)\n","          temperature = 0.9 # <--- Try a value between 0.5 and 1.0\n","\n","          # 1. Divide logits by temperature\n","          # 2. Apply softmax to get probabilities\n","          # 3. Use multinomial to sample one token based on the probabilities\n","          probabilities = torch.softmax(logits / temperature, dim=-1)\n","          next_token = torch.multinomial(probabilities, num_samples=1)\n","\n","          token_id = next_token.squeeze().item()\n","\n","          # Check for the End-of-Sequence token\n","          if token_id == eos_token_id:\n","              break\n","\n","          if token_id == 0 or token_id == sos_token_id:\n","              continue\n","\n","            # Append the predicted token\n","          generated_tokens.append(token_id)\n","\n","          # The predicted token becomes the input for the next iteration\n","          dec_input = next_token\n","\n","      # Return the list of generated token IDs\n","      return generated_tokens\n"],"metadata":{"id":"ORRhJ3QBpMQw"},"execution_count":null,"outputs":[]}]}