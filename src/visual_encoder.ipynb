{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3RhUIyQJCgYTB/vYSPNnW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BGoLKOn-okdP"},"outputs":[],"source":["class Backbone(nn.Module):\n","    \"\"\"\n","      Main convolutional blocks for our CNN\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(Backbone, self).__init__()\n","        # Encoder convolutional layers\n","        self.encoder_conv = nn.Sequential(\n","            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n","            nn.GroupNorm(8, 16),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n","            nn.GroupNorm(8, 32),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n","            nn.GroupNorm(8, 64),\n","            nn.LeakyReLU(0.1),\n","        )\n","\n","        # Calculate flattened dimension for linear layer\n","        self.flatten_dim = 64 * output_w * output_h\n","        # Latent space layers\n","        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n","\n","\n","    def forward(self, x):\n","        x = self.encoder_conv(x)\n","        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n","        z = self.fc1(x)\n","        return z\n","\n","class VisualEncoder(nn.Module):\n","    \"\"\"\n","      Encodes an image into a latent space representation. Note the two pathways\n","      to try to disentangle the mean pattern from the image\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualEncoder, self).__init__()\n","\n","        self.context_backbone = Backbone(latent_dim, output_w, output_h)\n","        self.content_backbone = Backbone(latent_dim, output_w, output_h)\n","\n","        self.projection = nn.Linear(2*latent_dim, latent_dim)\n","    def forward(self, x):\n","        z_context = self.context_backbone(x)\n","        z_content = self.content_backbone(x)\n","        z = torch.cat((z_content, z_context), dim=1)\n","        z = self.projection(z)\n","        return z"]}]}