{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMU2n6Z45B9WFf6T37dj+hW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n"],"metadata":{"id":"pS_rO9_C1fbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Backbone(nn.Module):\n","    \"\"\"\n","      Main convolutional blocks for our CNN\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(Backbone, self).__init__()\n","        # Encoder convolutional layers\n","        self.encoder_conv = nn.Sequential(\n","            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n","            nn.GroupNorm(8, 16),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n","            nn.GroupNorm(8, 32),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n","            nn.GroupNorm(8, 64),\n","            nn.LeakyReLU(0.1),\n","        )\n","\n","        # Calculate flattened dimension for linear layer\n","        self.flatten_dim = 64 * output_w * output_h\n","        # Latent space layers\n","        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n","\n","\n","    def forward(self, x):\n","        x = self.encoder_conv(x)\n","        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n","        z = self.fc1(x)\n","        return z\n","\n","class VisualEncoder(nn.Module):\n","    \"\"\"\n","      Encodes an image into a latent space representation. Note the two pathways\n","      to try to disentangle the mean pattern from the image\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualEncoder, self).__init__()\n","\n","        self.context_backbone = Backbone(latent_dim, output_w, output_h)\n","        self.content_backbone = Backbone(latent_dim, output_w, output_h)\n","\n","        self.projection = nn.Linear(2*latent_dim, latent_dim)\n","    def forward(self, x):\n","        z_context = self.context_backbone(x)\n","        z_content = self.content_backbone(x)\n","        z = torch.cat((z_content, z_context), dim=1)\n","        z = self.projection(z)\n","        return z"],"metadata":{"id":"CkQQ1ltalQaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLSTM(nn.Module):\n","    \"\"\"\n","      Encodes a sequence of tokens into a latent space representation.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","\n","    def forward(self, input_seq):\n","        embedded = self.embedding(input_seq)\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        return outputs, hidden, cell\n","\n","class DecoderLSTM(nn.Module):\n","    \"\"\"\n","      Decodes a latent space representation into a sequence of tokens.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n","\n","    def forward(self, input_seq, hidden, cell):\n","        embedded = self.embedding(input_seq)\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        prediction = self.out(output)\n","        return prediction, hidden, cell\n","\n","# We create the basic text autoencoder (a special case of a sequence to sequence model)\n","class Seq2SeqLSTM(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, target_seq):\n","        # input_seq and target_seq are both your 'input_ids'\n","        # 1. Encode the input sequence\n","        _enc_out, hidden, cell = self.encoder(input_seq)\n","\n","        # 2. Create the \"shifted\" decoder input for teacher forcing.\n","        # We want to predict target_seq[:, 1:]\n","        # So, we feed in target_seq[:, :-1]\n","        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n","        decoder_input = target_seq[:, :-1]\n","\n","        # 3. Run the decoder *once* on the entire sequence.\n","        # It takes the encoder's final state (hidden, cell)\n","        # and the full \"teacher\" sequence (decoder_input).\n","        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n","\n","        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n","        return predictions"],"metadata":{"id":"WpvxcOsqlWlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SequencePredictor(nn.Module):\n","    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim,\n","                 gru_hidden_size): # Renamed gru_hidden_dim to gru_hidden_size for clarity\n","        super(SequencePredictor, self).__init__()\n","\n","        # --- 1. Static Encoders ---\n","        # (These process one pair at a time)\n","        self.image_encoder = visual_autoencoder.encoder\n","        self.text_encoder = text_autoencoder.encoder\n","\n","        # --- 2. Temporal Encoder ---\n","        # (This processes the sequence of pairs)\n","        fusion_dim = latent_dim * 2 # z_visual + z_text\n","        self.temporal_rnn = nn.GRU(fusion_dim, gru_hidden_size, batch_first=True, bidirectional=True) # Changed to unidirectional\n","\n","        # The actual output dimension of the unidirectional GRU\n","        self.gru_output_dim = gru_hidden_size * 2# Changed for unidirectional\n","\n","        # --- 3. Attention ---\n","        self.attention = Attention(self.gru_output_dim)\n","\n","        # --- 4. Final Projection ---\n","        # Input to projection will be concatenated h_combined and context\n","        # h_combined (from GRU's h) will be gru_output_dim\n","        # context (from attention) will be gru_output_dim\n","        # So, input to projection is gru_output_dim * 2\n","        self.projection = nn.Sequential(\n","            nn.Linear(self.gru_output_dim * 2, latent_dim), # Corrected input size for unidirectional\n","            nn.ReLU()\n","        )\n","\n","        # --- 5. Decoders ---\n","        # (These predict the *next* item)\n","        self.image_decoder = visual_autoencoder.decoder\n","        self.text_decoder = text_autoencoder.decoder\n","\n","        # The hidden and cell states for text_decoder should match its own latent_dim\n","        # The text_decoder's LSTM is initialized with hidden_dim=latent_dim\n","        self.fused_to_h0 = nn.Linear(latent_dim, latent_dim) # Project to text_decoder's hidden_dim\n","        self.fused_to_c0 = nn.Linear(latent_dim, latent_dim) # Project to text_decoder's hidden_dim\n","\n","    def forward(self, image_seq, text_seq, target_seq):\n","        # image_seq shape: [batch, seq_len, C, H, W]\n","        # text_seq shape:  [batch, seq_len, text_len]\n","        # target_text_for_teacher_forcing: [batch, 1, text_len]\n","\n","        batch_size, seq_len, C, H, W = image_seq.shape\n","\n","        # --- 1 & 2: Run Static Encoders over the sequence ---\n","        # We can't pass a 5D/4D tensor to the encoders.\n","        # We \"flatten\" the batch and sequence dimensions.\n","\n","        # Reshape for image_encoder\n","        img_flat = image_seq.view(batch_size * seq_len, C, H, W)\n","        # Reshape for text_encoder\n","        txt_flat = text_seq.view(batch_size * seq_len, -1) # -1 infers text_len\n","\n","        # Run encoders\n","        z_v_flat = self.image_encoder(img_flat) # Shape: [b*s, latent]\n","        _, hidden_txt, cell_txt = self.text_encoder(txt_flat) # Renamed hidden, cell to hidden_txt, cell_txt for clarity\n","\n","        # Combine visual latent and text encoder's final hidden state\n","        # text_encoder's hidden output is [num_layers, b*s, latent_dim]\n","        # Assuming num_layers=1, this is [1, b*s, latent_dim]\n","        # Squeeze 0 to get [b*s, latent_dim]\n","        z_fusion_flat = torch.cat((z_v_flat, hidden_txt.squeeze(0)), dim=1) # Shape: [b*s, fusion_dim]\n","\n","        # \"Un-flatten\" back into a sequence\n","        z_fusion_seq = z_fusion_flat.view(batch_size, seq_len, -1) # Shape: [b, s, fusion_dim]\n","\n","        # --- 3. Run Temporal Encoder ---\n","        zseq, h = self.temporal_rnn(z_fusion_seq)\n","        # zseq shape: [batch, seq_len, gru_hidden_size]\n","        # h shape: [num_layers, batch, gru_hidden_size] -> [1, batch, gru_hidden_size] for num_layers=1\n","\n","        # For unidirectional GRU, h_combined is simply the squeezed hidden state\n","        #h_combined = h.squeeze(0) # Shape: [batch, gru_hidden_size]\n","        h = h.view(1, 2, batch_size, -1)  # → [1, 2, B, hidden]\n","\n","        h_forward = h[0, 0]     # [B, hidden]\n","        h_backward = h[0, 1]    # [B, hidden]\n","\n","        # Combine both directions → [B, hidden*2]\n","        h_combined = torch.cat((h_forward, h_backward), dim=-1)\n","\n","        # --- 4. Attention ---\n","        context = self.attention(zseq) # Shape: [b, gru_hidden_size]\n","\n","        # --- 5. Final Prediction Vector (z) ---\n","        # Concatenate h_combined and context\n","        z = self.projection(torch.cat((h_combined, context), dim=1)) # Input to projection is [batch, 2 * gru_hidden_size]\n","\n","        # --- 6. Decode (Predict pk) ---\n","        pred_image_content, pred_image_context = self.image_decoder(z)\n","\n","        # The initial hidden and cell states for the text decoder\n","        h0 = self.fused_to_h0(z).unsqueeze(0) # z is [batch, latent_dim], h0 becomes [1, batch, latent_dim]\n","        c0 = self.fused_to_c0(z).unsqueeze(0) # c0 becomes [1, batch, latent_dim]\n","\n","        # text_decoder expects input_seq, hidden, cell\n","        # target_seq has shape [batch, 1, 120]\n","        # decoder_input should be [batch_size, 119] for text_decoder's forward\n","        decoder_input = target_seq.squeeze(1)[:, :-1] # Corrected: target_seq is [batch, 1, 120] -> squeeze(1) -> [batch, 120] -> [:, :-1] -> [batch, 119]\n","\n","        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)\n","\n","        return pred_image_content, pred_image_context, predicted_text_logits_k, h0, c0"],"metadata":{"id":"eUcYJWfJll3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SequencePredictionDataset(Dataset):\n","    def __init__(self, original_dataset, tokenizer):\n","        super(SequencePredictionDataset, self).__init__()\n","        self.dataset = original_dataset\n","        self.tokenizer = tokenizer\n","        # Potential experiments: Try other transforms!\n","        self.transform = transforms.Compose([\n","          transforms.Resize((60, 125)),# Reasonable size based on our previous analysis\n","          transforms.ToTensor(), # HxWxC -> CxHxW\n","        ])\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","      \"\"\"\n","      Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one\n","      as a target.\n","      \"\"\"\n","      num_frames = self.dataset[idx][\"frame_count\"]\n","      frames = self.dataset[idx][\"images\"]\n","      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n","\n","      frame_tensors = []\n","      description_list = []\n","\n","      for frame_idx in range(4):\n","        image = FT.equalize(frames[frame_idx])\n","        input_frame = self.transform(image)\n","        frame_tensors.append(input_frame)\n","\n","        # Potential experiments: Try using the other attributes in your training\n","        # objects = self.image_attributes[frame_idx][\"objects\"]\n","        # actions = self.image_attributes[frame_idx][\"actions\"]\n","        # locations = self.image_attributes[frame_idx][\"locations\"]\n","\n","        description = self.image_attributes[frame_idx][\"description\"]\n","        # We need to return the tokens for NLP\n","        input_ids =  self.tokenizer(description,\n","                             return_tensors=\"pt\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=120).input_ids\n","\n","        description_list.append(input_ids.squeeze(0))\n","\n","\n","      image_target = FT.equalize(frames[4])\n","      image_target = self.transform(image_target)\n","      text_target = self.image_attributes[4][\"description\"]\n","\n","      target_ids = tokenizer(description,\n","                             return_tensors=\"pt\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=120).input_ids\n","\n","      sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)\n","      description_tensor = torch.stack(description_list) # (num_frames, max_length)\n","\n","      return (sequence_tensor, # Returning the image\n","              description_tensor, # Returning the whole description\n","              image_target, # Image target\n","              target_ids) # Text target\n"],"metadata":{"id":"mac6dVvXlviy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a03-TGWSPT1U"},"outputs":[],"source":["class VisualAutoencoder( nn.Module):\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualAutoencoder, self).__init__()\n","        self.encoder = VisualEncoder(latent_dim, output_w, output_h)\n","        self.decoder = VisualDecoder(latent_dim, output_w, output_h)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n"]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        # This \"attention\" layer learns a query vector\n","        self.attn = nn.Linear(hidden_dim, 1)\n","        self.softmax = nn.Softmax(dim=1) # Over the sequence length\n","\n","    def forward(self, rnn_outputs):\n","        # rnn_outputs shape: [batch, seq_len, hidden_dim]\n","\n","        # Pass through linear layer to get \"energy\" scores\n","        energy = self.attn(rnn_outputs).squeeze(2) # Shape: [batch, seq_len]\n","\n","        # Get attention weights\n","        attn_weights = self.softmax(energy) # Shape: [batch, seq_len]\n","\n","        # Apply weights\n","        # attn_weights.unsqueeze(1) -> [batch, 1, seq_len]\n","        # bmm with rnn_outputs -> [batch, 1, hidden_dim]\n","        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)\n","\n","        # Squeeze to get final context vector\n","        return context.squeeze(1) # Shape: [batch, hidden_dim]"],"metadata":{"id":"FB45BXzFP5gV"},"execution_count":null,"outputs":[]}]}