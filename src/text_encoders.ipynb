{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBtSrxCAkLAougn4oKM1qn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pPRn7zthqwSM"},"outputs":[],"source":["class EncoderLSTM(nn.Module):\n","    \"\"\"\n","      Encodes a sequence of tokens into a latent space representation.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","\n","    def forward(self, input_seq):\n","        embedded = self.embedding(input_seq)\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        return outputs, hidden, cell\n","\n","class DecoderLSTM(nn.Module):\n","    \"\"\"\n","      Decodes a latent space representation into a sequence of tokens.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n","\n","    def forward(self, input_seq, hidden, cell):\n","        embedded = self.embedding(input_seq)\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        prediction = self.out(output)\n","        return prediction, hidden, cell\n","\n","# We create the basic text autoencoder (a special case of a sequence to sequence model)\n","class Seq2SeqLSTM(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, target_seq):\n","        # input_seq and target_seq are both your 'input_ids'\n","        # 1. Encode the input sequence\n","        _enc_out, hidden, cell = self.encoder(input_seq)\n","\n","        # 2. Create the \"shifted\" decoder input for teacher forcing.\n","        # We want to predict target_seq[:, 1:]\n","        # So, we feed in target_seq[:, :-1]\n","        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n","        decoder_input = target_seq[:, :-1]\n","\n","        # 3. Run the decoder *once* on the entire sequence.\n","        # It takes the encoder's final state (hidden, cell)\n","        # and the full \"teacher\" sequence (decoder_input).\n","        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n","\n","        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n","        return predictions"]}]}