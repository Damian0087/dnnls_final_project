{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwPM1hyz4a564b4ZVmRWhy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5c66cf5adf65476c9c81b5c40f67e872":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15ac64a1a98542b0bde4eb14e75b9d01","IPY_MODEL_58437696ed5c4893b86d0884e791387f","IPY_MODEL_e450d11b92a94ddabf62ae72bcb13c4f"],"layout":"IPY_MODEL_ea888a2e79e346d690f2b2bdda6e31bf"}},"15ac64a1a98542b0bde4eb14e75b9d01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c94680fead94e588305bdf3b8c9ace3","placeholder":"​","style":"IPY_MODEL_eec3858e96da4527b2cf00fae81ba33a","value":"tokenizer_config.json: 100%"}},"58437696ed5c4893b86d0884e791387f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4d9dbcb71a040458838ac54bdd90997","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64ab529ef9d54d99b2451fef2f0c5550","value":48}},"e450d11b92a94ddabf62ae72bcb13c4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc279c04df8547f4aec024573e3731d3","placeholder":"​","style":"IPY_MODEL_b03f4bf610c943e4a890466a06e333e2","value":" 48.0/48.0 [00:00&lt;00:00, 904B/s]"}},"ea888a2e79e346d690f2b2bdda6e31bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c94680fead94e588305bdf3b8c9ace3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec3858e96da4527b2cf00fae81ba33a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4d9dbcb71a040458838ac54bdd90997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64ab529ef9d54d99b2451fef2f0c5550":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc279c04df8547f4aec024573e3731d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b03f4bf610c943e4a890466a06e333e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37ba460ea75e43b8a44d2ccd8e19d920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6fed8da6f3340328375c0a20583eb83","IPY_MODEL_8c67834af8c54d3097e42792bac78dc0","IPY_MODEL_45d82e17bf844bde968f18333b475be4"],"layout":"IPY_MODEL_b9c8090004804af3939fc68c91bb25e0"}},"e6fed8da6f3340328375c0a20583eb83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_832b382191574b8bb2e9653fdbb8b8d8","placeholder":"​","style":"IPY_MODEL_e0289e8af277486d82a10f37af645b17","value":"vocab.txt: "}},"8c67834af8c54d3097e42792bac78dc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc7a220ff66a47b984159ddc97141018","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a40c3b525894a8699f5009fd9a854d3","value":1}},"45d82e17bf844bde968f18333b475be4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51170c9310ca4563a49b9d9d727d2439","placeholder":"​","style":"IPY_MODEL_d4aa283843bb456092b09acdcc97a96d","value":" 232k/? [00:00&lt;00:00, 6.03MB/s]"}},"b9c8090004804af3939fc68c91bb25e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"832b382191574b8bb2e9653fdbb8b8d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0289e8af277486d82a10f37af645b17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc7a220ff66a47b984159ddc97141018":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"0a40c3b525894a8699f5009fd9a854d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51170c9310ca4563a49b9d9d727d2439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4aa283843bb456092b09acdcc97a96d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e5573168c9e485caac942199f8ebfbc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe8be9e1b69f4b1db57116b897d10d3e","IPY_MODEL_08c0294459b046058784aae5bc20a1d1","IPY_MODEL_ccaf2b901fa544b78b7e6dba8444dca1"],"layout":"IPY_MODEL_0f05e890fc0d4fc5a6c6f0ef2b371b7a"}},"fe8be9e1b69f4b1db57116b897d10d3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d61e6c8641894f9594d99182e9b40579","placeholder":"​","style":"IPY_MODEL_62a35af8690c40a7b0ece427e3671ef1","value":"tokenizer.json: "}},"08c0294459b046058784aae5bc20a1d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f6048c5029b4372826f8fde59c233b1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e608ee55a50418b957fd5affa0b33d5","value":1}},"ccaf2b901fa544b78b7e6dba8444dca1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7144fa6678804b51bbd9e34730ac832d","placeholder":"​","style":"IPY_MODEL_269fa9c6ede84a09839450e93189eecd","value":" 466k/? [00:00&lt;00:00, 15.5MB/s]"}},"0f05e890fc0d4fc5a6c6f0ef2b371b7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d61e6c8641894f9594d99182e9b40579":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62a35af8690c40a7b0ece427e3671ef1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f6048c5029b4372826f8fde59c233b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7e608ee55a50418b957fd5affa0b33d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7144fa6678804b51bbd9e34730ac832d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"269fa9c6ede84a09839450e93189eecd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9543c2b5bafb4dfd80b14ebbc6785b62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0153515364d46cf9cbc23f5daee65c8","IPY_MODEL_10235fb62f87430598a96e5d572b56a9","IPY_MODEL_849f8b986cf049c19ce30b2f58030335"],"layout":"IPY_MODEL_b7fe195c45d047e8966f40a943c9d26f"}},"e0153515364d46cf9cbc23f5daee65c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf63b5647bd64676818675feb1928d80","placeholder":"​","style":"IPY_MODEL_bcfc77e957504b18aa682d6baa05dfa3","value":"config.json: 100%"}},"10235fb62f87430598a96e5d572b56a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09e18cc4c78d471fb0062910bd2dc0a4","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ab5303876574db18d15171f4983ace7","value":570}},"849f8b986cf049c19ce30b2f58030335":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2728f85c07464e7f90c0fb6579b51485","placeholder":"​","style":"IPY_MODEL_5e03269bc6e14eb6894e497a0036b3d3","value":" 570/570 [00:00&lt;00:00, 42.3kB/s]"}},"b7fe195c45d047e8966f40a943c9d26f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf63b5647bd64676818675feb1928d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcfc77e957504b18aa682d6baa05dfa3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09e18cc4c78d471fb0062910bd2dc0a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ab5303876574db18d15171f4983ace7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2728f85c07464e7f90c0fb6579b51485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e03269bc6e14eb6894e497a0036b3d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from transformers import BertTokenizer\n","import torchvision.transforms.functional as FT\n","\n","from src.utils import parse_gdi_text\n","\n","\n"],"metadata":{"id":"pS_rO9_C1fbZ","executionInfo":{"status":"error","timestamp":1767198811141,"user_tz":0,"elapsed":45,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}},"colab":{"base_uri":"https://localhost:8080/","height":383},"outputId":"c87344b1-0995-41ba-e633-91d6b98ff6f5"},"execution_count":9,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'src'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1198457809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_gdi_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["class Backbone(nn.Module):\n","    \"\"\"\n","      Main convolutional blocks for our CNN\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(Backbone, self).__init__()\n","        # Encoder convolutional layers\n","        self.encoder_conv = nn.Sequential(\n","            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n","            nn.GroupNorm(8, 16),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n","            nn.GroupNorm(8, 32),\n","            nn.LeakyReLU(0.1),\n","\n","            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n","            nn.GroupNorm(8, 64),\n","            nn.LeakyReLU(0.1),\n","        )\n","\n","        # Calculate flattened dimension for linear layer\n","        self.flatten_dim = 64 * output_w * output_h\n","        # Latent space layers\n","        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n","\n","\n","    def forward(self, x):\n","        x = self.encoder_conv(x)\n","        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n","        z = self.fc1(x)\n","        return z\n","\n","class VisualEncoder(nn.Module):\n","    \"\"\"\n","      Encodes an image into a latent space representation. Note the two pathways\n","      to try to disentangle the mean pattern from the image\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualEncoder, self).__init__()\n","\n","        self.context_backbone = Backbone(latent_dim, output_w, output_h)\n","        self.content_backbone = Backbone(latent_dim, output_w, output_h)\n","\n","        self.projection = nn.Linear(2*latent_dim, latent_dim)\n","    def forward(self, x):\n","        z_context = self.context_backbone(x)\n","        z_content = self.content_backbone(x)\n","        z = torch.cat((z_content, z_context), dim=1)\n","        z = self.projection(z)\n","        return z"],"metadata":{"id":"CkQQ1ltalQaq","executionInfo":{"status":"ok","timestamp":1767197441134,"user_tz":0,"elapsed":44,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class EncoderLSTM(nn.Module):\n","    \"\"\"\n","      Encodes a sequence of tokens into a latent space representation.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","\n","    def forward(self, input_seq):\n","        embedded = self.embedding(input_seq)\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        return outputs, hidden, cell\n","\n","class DecoderLSTM(nn.Module):\n","    \"\"\"\n","      Decodes a latent space representation into a sequence of tokens.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n","                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n","        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n","\n","    def forward(self, input_seq, hidden, cell):\n","        embedded = self.embedding(input_seq)\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        prediction = self.out(output)\n","        return prediction, hidden, cell\n","\n","# We create the basic text autoencoder (a special case of a sequence to sequence model)\n","class Seq2SeqLSTM(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, target_seq):\n","        # input_seq and target_seq are both your 'input_ids'\n","        # 1. Encode the input sequence\n","        _enc_out, hidden, cell = self.encoder(input_seq)\n","\n","        # 2. Create the \"shifted\" decoder input for teacher forcing.\n","        # We want to predict target_seq[:, 1:]\n","        # So, we feed in target_seq[:, :-1]\n","        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n","        decoder_input = target_seq[:, :-1]\n","\n","        # 3. Run the decoder *once* on the entire sequence.\n","        # It takes the encoder's final state (hidden, cell)\n","        # and the full \"teacher\" sequence (decoder_input).\n","        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n","\n","        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n","        return predictions"],"metadata":{"id":"WpvxcOsqlWlx","executionInfo":{"status":"ok","timestamp":1767197441227,"user_tz":0,"elapsed":69,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class VisualDecoder(nn.Module):\n","    \"\"\"\n","      Decodes a latent representation into a content image and a context image\n","    \"\"\"\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualDecoder, self).__init__()\n","        self.imh = 60\n","        self.imw = 125\n","        self.flatten_dim = 64 * output_w * output_h\n","        self.output_w = output_w\n","        self.output_h = output_h\n","\n","        self.fc1 = nn.Linear(latent_dim, self.flatten_dim)\n","\n","        self.decoder_conv = nn.Sequential(\n","          nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,1)),\n","          nn.GroupNorm(8, 32),\n","          nn.LeakyReLU(0.1),\n","\n","          nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n","          nn.GroupNorm(8, 16),\n","          nn.LeakyReLU(0.1),\n","\n","          nn.ConvTranspose2d(16, 3, kernel_size=7, stride=2, padding=3, output_padding=(1, 1)),\n","          nn.Sigmoid() # Use nn.Tanh() if your data is normalized to [-1, 1]\n","      )\n","\n","    def forward(self, z):\n","      x = self.fc1(z)\n","\n","      x_content = self.decode_image(x)\n","      x_context = self.decode_image(x)\n","\n","      return x_content, x_context\n","\n","    def decode_image(self, x):\n","      x = x.view(-1, 64, self.output_w, self.output_h)      # reshape to conv feature map\n","      x = self.decoder_conv(x)\n","      x = x[:, :, :self.imh, :self.imw]          # crop to original size if needed\n","      return x\n"],"metadata":{"id":"vk6Bwv7nUfVU","executionInfo":{"status":"ok","timestamp":1767197441311,"user_tz":0,"elapsed":79,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class SequencePredictor(nn.Module):\n","    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim,\n","                 gru_hidden_size): # Renamed gru_hidden_dim to gru_hidden_size for clarity\n","        super(SequencePredictor, self).__init__()\n","\n","        # --- 1. Static Encoders ---\n","        # (These process one pair at a time)\n","        self.image_encoder = visual_autoencoder.encoder\n","        self.text_encoder = text_autoencoder.encoder\n","\n","        # --- 2. Temporal Encoder ---\n","        # (This processes the sequence of pairs)\n","        fusion_dim = latent_dim * 2 # z_visual + z_text\n","        self.temporal_rnn = nn.GRU(fusion_dim, gru_hidden_size, batch_first=True, bidirectional=True) # Changed to unidirectional\n","\n","        # The actual output dimension of the unidirectional GRU\n","        self.gru_output_dim = gru_hidden_size * 2# Changed for unidirectional\n","\n","        # --- 3. Attention ---\n","        self.attention = Attention(self.gru_output_dim)\n","\n","        # --- 4. Final Projection ---\n","        # Input to projection will be concatenated h_combined and context\n","        # h_combined (from GRU's h) will be gru_output_dim\n","        # context (from attention) will be gru_output_dim\n","        # So, input to projection is gru_output_dim * 2\n","        self.projection = nn.Sequential(\n","            nn.Linear(self.gru_output_dim * 2, latent_dim), # Corrected input size for unidirectional\n","            nn.ReLU()\n","        )\n","\n","        # --- 5. Decoders ---\n","        # (These predict the *next* item)\n","        self.image_decoder = visual_autoencoder.decoder\n","        self.text_decoder = text_autoencoder.decoder\n","\n","        # The hidden and cell states for text_decoder should match its own latent_dim\n","        # The text_decoder's LSTM is initialized with hidden_dim=latent_dim\n","        self.fused_to_h0 = nn.Linear(latent_dim, latent_dim) # Project to text_decoder's hidden_dim\n","        self.fused_to_c0 = nn.Linear(latent_dim, latent_dim) # Project to text_decoder's hidden_dim\n","\n","    def forward(self, image_seq, text_seq, target_seq):\n","        # image_seq shape: [batch, seq_len, C, H, W]\n","        # text_seq shape:  [batch, seq_len, text_len]\n","        # target_text_for_teacher_forcing: [batch, 1, text_len]\n","\n","        batch_size, seq_len, C, H, W = image_seq.shape\n","\n","        # --- 1 & 2: Run Static Encoders over the sequence ---\n","        # We can't pass a 5D/4D tensor to the encoders.\n","        # We \"flatten\" the batch and sequence dimensions.\n","\n","        # Reshape for image_encoder\n","        img_flat = image_seq.view(batch_size * seq_len, C, H, W)\n","        # Reshape for text_encoder\n","        txt_flat = text_seq.view(batch_size * seq_len, -1) # -1 infers text_len\n","\n","        # Run encoders\n","        z_v_flat = self.image_encoder(img_flat) # Shape: [b*s, latent]\n","        _, hidden_txt, cell_txt = self.text_encoder(txt_flat) # Renamed hidden, cell to hidden_txt, cell_txt for clarity\n","\n","        # Combine visual latent and text encoder's final hidden state\n","        # text_encoder's hidden output is [num_layers, b*s, latent_dim]\n","        # Assuming num_layers=1, this is [1, b*s, latent_dim]\n","        # Squeeze 0 to get [b*s, latent_dim]\n","        z_fusion_flat = torch.cat((z_v_flat, hidden_txt.squeeze(0)), dim=1) # Shape: [b*s, fusion_dim]\n","\n","        # \"Un-flatten\" back into a sequence\n","        z_fusion_seq = z_fusion_flat.view(batch_size, seq_len, -1) # Shape: [b, s, fusion_dim]\n","\n","        # --- 3. Run Temporal Encoder ---\n","        zseq, h = self.temporal_rnn(z_fusion_seq)\n","        # zseq shape: [batch, seq_len, gru_hidden_size]\n","        # h shape: [num_layers, batch, gru_hidden_size] -> [1, batch, gru_hidden_size] for num_layers=1\n","\n","        # For unidirectional GRU, h_combined is simply the squeezed hidden state\n","        #h_combined = h.squeeze(0) # Shape: [batch, gru_hidden_size]\n","        h = h.view(1, 2, batch_size, -1)  # → [1, 2, B, hidden]\n","\n","        h_forward = h[0, 0]     # [B, hidden]\n","        h_backward = h[0, 1]    # [B, hidden]\n","\n","        # Combine both directions → [B, hidden*2]\n","        h_combined = torch.cat((h_forward, h_backward), dim=-1)\n","\n","        # --- 4. Attention ---\n","        context = self.attention(zseq) # Shape: [b, gru_hidden_size]\n","\n","        # --- 5. Final Prediction Vector (z) ---\n","        # Concatenate h_combined and context\n","        z = self.projection(torch.cat((h_combined, context), dim=1)) # Input to projection is [batch, 2 * gru_hidden_size]\n","\n","        # --- 6. Decode (Predict pk) ---\n","        pred_image_content, pred_image_context = self.image_decoder(z)\n","\n","        # The initial hidden and cell states for the text decoder\n","        h0 = self.fused_to_h0(z).unsqueeze(0) # z is [batch, latent_dim], h0 becomes [1, batch, latent_dim]\n","        c0 = self.fused_to_c0(z).unsqueeze(0) # c0 becomes [1, batch, latent_dim]\n","\n","        # text_decoder expects input_seq, hidden, cell\n","        # target_seq has shape [batch, 1, 120]\n","        # decoder_input should be [batch_size, 119] for text_decoder's forward\n","        decoder_input = target_seq.squeeze(1)[:, :-1] # Corrected: target_seq is [batch, 1, 120] -> squeeze(1) -> [batch, 120] -> [:, :-1] -> [batch, 119]\n","\n","        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)\n","\n","        return pred_image_content, pred_image_context, predicted_text_logits_k, h0, c0"],"metadata":{"id":"eUcYJWfJll3p","executionInfo":{"status":"ok","timestamp":1767197441353,"user_tz":0,"elapsed":49,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["5c66cf5adf65476c9c81b5c40f67e872","15ac64a1a98542b0bde4eb14e75b9d01","58437696ed5c4893b86d0884e791387f","e450d11b92a94ddabf62ae72bcb13c4f","ea888a2e79e346d690f2b2bdda6e31bf","5c94680fead94e588305bdf3b8c9ace3","eec3858e96da4527b2cf00fae81ba33a","f4d9dbcb71a040458838ac54bdd90997","64ab529ef9d54d99b2451fef2f0c5550","fc279c04df8547f4aec024573e3731d3","b03f4bf610c943e4a890466a06e333e2","37ba460ea75e43b8a44d2ccd8e19d920","e6fed8da6f3340328375c0a20583eb83","8c67834af8c54d3097e42792bac78dc0","45d82e17bf844bde968f18333b475be4","b9c8090004804af3939fc68c91bb25e0","832b382191574b8bb2e9653fdbb8b8d8","e0289e8af277486d82a10f37af645b17","cc7a220ff66a47b984159ddc97141018","0a40c3b525894a8699f5009fd9a854d3","51170c9310ca4563a49b9d9d727d2439","d4aa283843bb456092b09acdcc97a96d","9e5573168c9e485caac942199f8ebfbc","fe8be9e1b69f4b1db57116b897d10d3e","08c0294459b046058784aae5bc20a1d1","ccaf2b901fa544b78b7e6dba8444dca1","0f05e890fc0d4fc5a6c6f0ef2b371b7a","d61e6c8641894f9594d99182e9b40579","62a35af8690c40a7b0ece427e3671ef1","0f6048c5029b4372826f8fde59c233b1","7e608ee55a50418b957fd5affa0b33d5","7144fa6678804b51bbd9e34730ac832d","269fa9c6ede84a09839450e93189eecd","9543c2b5bafb4dfd80b14ebbc6785b62","e0153515364d46cf9cbc23f5daee65c8","10235fb62f87430598a96e5d572b56a9","849f8b986cf049c19ce30b2f58030335","b7fe195c45d047e8966f40a943c9d26f","bf63b5647bd64676818675feb1928d80","bcfc77e957504b18aa682d6baa05dfa3","09e18cc4c78d471fb0062910bd2dc0a4","5ab5303876574db18d15171f4983ace7","2728f85c07464e7f90c0fb6579b51485","5e03269bc6e14eb6894e497a0036b3d3"]},"id":"xqmySE66UAWK","executionInfo":{"status":"ok","timestamp":1766612579439,"user_tz":0,"elapsed":5757,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}},"outputId":"89256eff-895f-4b47-b2fc-8b88aba42741"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c66cf5adf65476c9c81b5c40f67e872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ba460ea75e43b8a44d2ccd8e19d920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5573168c9e485caac942199f8ebfbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9543c2b5bafb4dfd80b14ebbc6785b62"}},"metadata":{}}]},{"cell_type":"code","source":["class SequencePredictionDataset(Dataset):\n","    def __init__(self, original_dataset, tokenizer):\n","        super(SequencePredictionDataset, self).__init__()\n","        self.dataset = original_dataset\n","        self.tokenizer = tokenizer\n","        # Potential experiments: Try other transforms!\n","        self.transform = transforms.Compose([\n","          transforms.Resize((60, 125)),# Reasonable size based on our previous analysis\n","          transforms.ToTensor(), # HxWxC -> CxHxW\n","        ])\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","      \"\"\"\n","      Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one\n","      as a target.\n","      \"\"\"\n","      num_frames = self.dataset[idx][\"frame_count\"]\n","      frames = self.dataset[idx][\"images\"]\n","      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n","\n","      frame_tensors = []\n","      description_list = []\n","\n","      for frame_idx in range(4):\n","        image = FT.equalize(frames[frame_idx])\n","        input_frame = self.transform(image)\n","        frame_tensors.append(input_frame)\n","\n","        # Potential experiments: Try using the other attributes in your training\n","        # objects = self.image_attributes[frame_idx][\"objects\"]\n","        # actions = self.image_attributes[frame_idx][\"actions\"]\n","        # locations = self.image_attributes[frame_idx][\"locations\"]\n","\n","        description = self.image_attributes[frame_idx][\"description\"]\n","        # We need to return the tokens for NLP\n","        input_ids =  self.tokenizer(description,\n","                             return_tensors=\"pt\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=120).input_ids\n","\n","        description_list.append(input_ids.squeeze(0))\n","\n","\n","      image_target = FT.equalize(frames[4])\n","      image_target = self.transform(image_target)\n","      text_target = self.image_attributes[4][\"description\"]\n","\n","      target_ids = tokenizer(description,\n","                             return_tensors=\"pt\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=120).input_ids\n","\n","      sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)\n","      description_tensor = torch.stack(description_list) # (num_frames, max_length)\n","\n","      return (sequence_tensor, # Returning the image\n","              description_tensor, # Returning the whole description\n","              image_target, # Image target\n","              target_ids) # Text target\n"],"metadata":{"id":"mac6dVvXlviy","executionInfo":{"status":"ok","timestamp":1767197441403,"user_tz":0,"elapsed":47,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"a03-TGWSPT1U","executionInfo":{"status":"ok","timestamp":1767197444479,"user_tz":0,"elapsed":52,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"outputs":[],"source":["class VisualAutoencoder( nn.Module):\n","    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n","        super(VisualAutoencoder, self).__init__()\n","        self.encoder = VisualEncoder(latent_dim, output_w, output_h)\n","        self.decoder = VisualDecoder(latent_dim, output_w, output_h)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n"]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        # This \"attention\" layer learns a query vector\n","        self.attn = nn.Linear(hidden_dim, 1)\n","        self.softmax = nn.Softmax(dim=1) # Over the sequence length\n","\n","    def forward(self, rnn_outputs):\n","        # rnn_outputs shape: [batch, seq_len, hidden_dim]\n","\n","        # Pass through linear layer to get \"energy\" scores\n","        energy = self.attn(rnn_outputs).squeeze(2) # Shape: [batch, seq_len]\n","\n","        # Get attention weights\n","        attn_weights = self.softmax(energy) # Shape: [batch, seq_len]\n","\n","        # Apply weights\n","        # attn_weights.unsqueeze(1) -> [batch, 1, seq_len]\n","        # bmm with rnn_outputs -> [batch, 1, hidden_dim]\n","        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)\n","\n","        # Squeeze to get final context vector\n","        return context.squeeze(1) # Shape: [batch, hidden_dim]"],"metadata":{"id":"FB45BXzFP5gV","executionInfo":{"status":"ok","timestamp":1767197445796,"user_tz":0,"elapsed":18,"user":{"displayName":"UGOCHUKWU TONY-OKONTA","userId":"00828561908584827048"}}},"execution_count":8,"outputs":[]}]}