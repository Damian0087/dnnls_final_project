{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOQWO//mSBThTvua19KRtk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Experiments\n","\n","This section documents the experimental runs carried out for this project.\n","All experiments were trained using the same dataset and preprocessing pipeline.\n","Only architectural changes were introduced between experiments.\n","\n","---\n","\n","## Experiment 1: Baseline Model\n","\n","**Architecture**\n","- Visual encoder: Autoencoder\n","- Text encoder: LSTM\n","- Temporal model: Unidirectional GRU\n","- Decoder: LSTM\n","\n","**Training setup**\n","- Optimizer: Adam\n","- Learning rate: (same as baseline code)\n","- Epochs: 5\n","- Teacher forcing enabled\n","\n","**Objective**\n","Establish a baseline for multimodal storytelling performance.\n","\n","**Outputs**\n","- Training loss curve\n","- Validation loss curve\n","- BLEU and Perplexity trends\n","\n","**Figures**\n","- `results/baseline_results.ipynb`\n","\n","\n","---\n","\n","## Experiment 2: Bidirectional Temporal Model\n","\n","**Modification**\n","- The temporal GRU was replaced with a bidirectional GRU\n","- All other components and hyperparameters remained unchanged\n","\n","**Motivation**\n","Bidirectional recurrent models can leverage both past and future context,\n","which may improve sequence modelling in storytelling tasks.\n","\n","**Training setup**\n","- Same as baseline\n","- Epochs: 5\n","\n","**Outputs**\n","- Training loss curve\n","- Validation loss curve\n","- BLEU and Perplexity trends\n","\n","**Figures**\n","- `results/bidirectional_results.ipynb`\n","\n","\n","---\n","\n","\n"],"metadata":{"id":"Vkm-I6CTKpZK"}}]}